{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a16de1",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7eb0344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device\n",
    "device = device = torch.device('mps')\n",
    "print(device)\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(1)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class AlexNetRelu(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetRelu, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 6 * 6, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "class AlexNetSigmoid(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetSigmoid, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 6 * 6, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "      \n",
    "class AlexNetTanh(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetTanh, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 6 * 6, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "modelSigmoid = AlexNetSigmoid(num_classes=num_classes).to(device)\n",
    "modelRelu = AlexNetRelu(num_classes=num_classes).to(device)\n",
    "modelTanh = AlexNetTanh(num_classes=num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369aada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "\n",
    "        logits = model(features)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        \n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(features)\n",
    "            probas = F.softmax(logits, dim=1)\n",
    "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "    \n",
    "def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        ##print(\"Before flatten:\", x.size())\n",
    "        x = torch.flatten(x, 1)\n",
    "        ##print(\"After flatten:\", x.size())\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "def eval(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            logits = model(features)\n",
    "            probas = F.softmax(logits, dim=1)\n",
    "            cost = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cost.backward()\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            ### LOGGING\n",
    "            if not batch_idx % 50:\n",
    "                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                       %(epoch+1, num_epochs, batch_idx, \n",
    "                         len(train_loader), cost))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False): # save memory during inference\n",
    "            print('Epoch: %03d/%03d | Train: %.3f%% |  Loss: %.3f' % (\n",
    "                  epoch+1, num_epochs, \n",
    "                  compute_accuracy(model, train_loader),\n",
    "                  compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cce6bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3015\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3018\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.2980\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.3004\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.2928\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.2940\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.2438\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.1784\n",
      "Epoch: 001/010 | Train: 25.468% |  Loss: 2.071\n",
      "Time elapsed: 2.06 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.0876\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.0069\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.9521\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.0061\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.9575\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.8306\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.7768\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.7122\n",
      "Epoch: 002/010 | Train: 38.664% |  Loss: 1.719\n",
      "Time elapsed: 4.01 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.8211\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.7608\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.5944\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.6367\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.6652\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.6188\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.5668\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.6482\n",
      "Epoch: 003/010 | Train: 44.072% |  Loss: 1.551\n",
      "Time elapsed: 5.94 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.5249\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.4207\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.6316\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.5505\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.5403\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.5167\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.2979\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.4294\n",
      "Epoch: 004/010 | Train: 48.320% |  Loss: 1.462\n",
      "Time elapsed: 8.05 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.5075\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.5250\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.4553\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.4262\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.4535\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.2496\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.2103\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.2408\n",
      "Epoch: 005/010 | Train: 52.300% |  Loss: 1.341\n",
      "Time elapsed: 10.06 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.3040\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.3378\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.3310\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.1267\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.3169\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.1326\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.2182\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.3888\n",
      "Epoch: 006/010 | Train: 56.894% |  Loss: 1.222\n",
      "Time elapsed: 12.11 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.2813\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.1809\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.1738\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 1.1941\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.1688\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.1936\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.2705\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.1490\n",
      "Epoch: 007/010 | Train: 58.792% |  Loss: 1.167\n",
      "Time elapsed: 14.19 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.2116\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.0499\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 1.4646\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.9835\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 1.1204\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 1.1675\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 1.2819\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.1423\n",
      "Epoch: 008/010 | Train: 61.188% |  Loss: 1.111\n",
      "Time elapsed: 16.17 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 1.1348\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.9788\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 1.0513\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 1.0016\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.1089\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.9911\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.9706\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 1.0312\n",
      "Epoch: 009/010 | Train: 64.648% |  Loss: 1.016\n",
      "Time elapsed: 18.18 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 1.1401\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 1.0506\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.9577\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 1.0126\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 1.0169\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 1.0315\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 1.1422\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.9442\n",
      "Epoch: 010/010 | Train: 68.250% |  Loss: 0.920\n",
      "Time elapsed: 20.15 min\n",
      "Total Training Time: 20.15 min\n",
      "Test accuracy: 66.53%\n"
     ]
    }
   ],
   "source": [
    "eval(modelRelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3685edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3431\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3584\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.3350\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.3070\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.2833\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.3252\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.3666\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.3628\n",
      "Epoch: 001/010 | Train: 10.000% |  Loss: 2.327\n",
      "Time elapsed: 2.02 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.3033\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.3467\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 2.2835\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.3366\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 2.3365\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 2.3716\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 2.3034\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 2.3088\n",
      "Epoch: 002/010 | Train: 10.000% |  Loss: 2.331\n",
      "Time elapsed: 4.04 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 2.3639\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 2.3182\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 2.3289\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 2.3202\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 2.3285\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 2.3549\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 2.3568\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 2.3634\n",
      "Epoch: 003/010 | Train: 10.000% |  Loss: 2.320\n",
      "Time elapsed: 6.17 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 2.3116\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 2.3174\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 2.3073\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 2.3181\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 2.3057\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 2.2919\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 2.3077\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 2.2998\n",
      "Epoch: 004/010 | Train: 10.000% |  Loss: 2.314\n",
      "Time elapsed: 8.19 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 2.3426\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 2.3232\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 2.2937\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 2.3445\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 2.3251\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 2.3086\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 2.3077\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 2.3148\n",
      "Epoch: 005/010 | Train: 10.000% |  Loss: 2.316\n",
      "Time elapsed: 10.23 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 2.3203\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 2.3231\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 2.3070\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 2.3261\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 2.3118\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 2.3118\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 2.3013\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 2.2914\n",
      "Epoch: 006/010 | Train: 10.000% |  Loss: 2.309\n",
      "Time elapsed: 12.09 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 2.3111\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 2.2994\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 2.2979\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 2.3087\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 2.3104\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 2.3302\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 2.3060\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 2.3052\n",
      "Epoch: 007/010 | Train: 10.000% |  Loss: 2.309\n",
      "Time elapsed: 13.95 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 2.3134\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 2.3189\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 2.2947\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 2.3148\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 2.2949\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 2.2952\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 2.3111\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 2.2952\n",
      "Epoch: 008/010 | Train: 10.000% |  Loss: 2.310\n",
      "Time elapsed: 15.82 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 2.3032\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 2.3080\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 2.3021\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 2.3074\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 2.3060\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 2.2947\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 2.3004\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 2.3144\n",
      "Epoch: 009/010 | Train: 10.000% |  Loss: 2.306\n",
      "Time elapsed: 17.87 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 2.2987\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 2.2965\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 2.2963\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 2.3027\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 2.3053\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 2.3110\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 2.3267\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 2.2987\n",
      "Epoch: 010/010 | Train: 10.000% |  Loss: 2.313\n",
      "Time elapsed: 19.78 min\n",
      "Total Training Time: 19.78 min\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "eval(modelSigmoid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5896d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3024\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.2687\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.0921\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.0106\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.8714\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.8325\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.8225\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.7375\n",
      "Epoch: 001/010 | Train: 37.052% |  Loss: 1.774\n",
      "Time elapsed: 1.90 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.8730\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.6398\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.7554\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.5894\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.5888\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.4689\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.5169\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.5826\n",
      "Epoch: 002/010 | Train: 46.202% |  Loss: 1.494\n",
      "Time elapsed: 3.76 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.4190\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.3952\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.6163\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.4687\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.5630\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.4110\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.3496\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.5385\n",
      "Epoch: 003/010 | Train: 51.926% |  Loss: 1.366\n",
      "Time elapsed: 5.88 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.4032\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.2196\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.2547\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.3499\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.3967\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.2460\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.2487\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.2717\n",
      "Epoch: 004/010 | Train: 56.050% |  Loss: 1.246\n",
      "Time elapsed: 7.75 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.4190\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.1856\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.1822\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.2033\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.1363\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.2849\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.2740\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.2909\n",
      "Epoch: 005/010 | Train: 58.340% |  Loss: 1.186\n",
      "Time elapsed: 9.67 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.2446\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.0911\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.1912\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.0492\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.0896\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.1849\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.1627\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.0588\n",
      "Epoch: 006/010 | Train: 62.036% |  Loss: 1.094\n",
      "Time elapsed: 11.56 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.0932\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.1040\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.2025\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 1.1173\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.9488\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.0467\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.0410\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.1054\n",
      "Epoch: 007/010 | Train: 63.958% |  Loss: 1.043\n",
      "Time elapsed: 13.46 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.0220\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.0828\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 1.0342\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.9381\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.9568\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.9244\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 1.0302\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.9840\n",
      "Epoch: 008/010 | Train: 66.646% |  Loss: 0.966\n",
      "Time elapsed: 15.48 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.8828\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 1.1537\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.8083\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 1.1159\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.8730\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.8572\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.8351\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.8161\n",
      "Epoch: 009/010 | Train: 67.192% |  Loss: 0.944\n",
      "Time elapsed: 17.43 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 1.0236\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.9263\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.9664\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.9029\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.9243\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.9536\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.9422\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.8757\n",
      "Epoch: 010/010 | Train: 69.962% |  Loss: 0.875\n",
      "Time elapsed: 19.62 min\n",
      "Total Training Time: 19.62 min\n",
      "Test accuracy: 68.05%\n"
     ]
    }
   ],
   "source": [
    "eval(modelTanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc494f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalAdam(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            logits = model(features)\n",
    "            probas = F.softmax(logits, dim=1)\n",
    "            cost = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cost.backward()\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            ### LOGGING\n",
    "            if not batch_idx % 50:\n",
    "                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                       %(epoch+1, num_epochs, batch_idx, \n",
    "                         len(train_loader), cost))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False): # save memory during inference\n",
    "            print('Epoch: %03d/%03d | Train: %.3f%% |  Loss: %.3f' % (\n",
    "                  epoch+1, num_epochs, \n",
    "                  compute_accuracy(model, train_loader),\n",
    "                  compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4c1127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 0.8890\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 1.8792\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.5043\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.5438\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.4740\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.3168\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.2551\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.2964\n",
      "Epoch: 001/010 | Train: 55.344% |  Loss: 1.246\n",
      "Time elapsed: 2.29 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.2581\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.2982\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.4488\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.0405\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.1611\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.0967\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.0256\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.1020\n",
      "Epoch: 002/010 | Train: 66.298% |  Loss: 0.968\n",
      "Time elapsed: 4.26 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.1085\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.1378\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 0.8829\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.7709\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 0.9576\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.0472\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.0667\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 0.8022\n",
      "Epoch: 003/010 | Train: 72.402% |  Loss: 0.791\n",
      "Time elapsed: 6.19 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.7116\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 0.9959\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 0.9352\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 0.8876\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 0.7665\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 0.8469\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 0.7746\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 0.9376\n",
      "Epoch: 004/010 | Train: 73.830% |  Loss: 0.753\n",
      "Time elapsed: 8.08 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.7427\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.6383\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 0.8310\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 0.7927\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.6104\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.8701\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 0.7789\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.7346\n",
      "Epoch: 005/010 | Train: 76.206% |  Loss: 0.679\n",
      "Time elapsed: 10.17 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.6567\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.6746\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.6375\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.7135\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.6477\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.6604\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.9241\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.8426\n",
      "Epoch: 006/010 | Train: 81.146% |  Loss: 0.563\n",
      "Time elapsed: 12.19 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.5514\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.5929\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.5604\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.6269\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.6805\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.7048\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.5979\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.6190\n",
      "Epoch: 007/010 | Train: 81.944% |  Loss: 0.519\n",
      "Time elapsed: 14.29 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.6361\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.6690\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.4628\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.4355\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.5510\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.6129\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.5923\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.7799\n",
      "Epoch: 008/010 | Train: 84.558% |  Loss: 0.441\n",
      "Time elapsed: 16.43 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.4302\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.4934\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.3246\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.4198\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.4586\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.4345\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.4819\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.4373\n",
      "Epoch: 009/010 | Train: 86.948% |  Loss: 0.379\n",
      "Time elapsed: 18.49 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.2991\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.6205\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.3971\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.3618\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.4996\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.4719\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.5928\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.5088\n",
      "Epoch: 010/010 | Train: 88.796% |  Loss: 0.325\n",
      "Time elapsed: 20.60 min\n",
      "Total Training Time: 20.60 min\n",
      "Test accuracy: 74.15%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelRelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5147e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3242\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3015\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.3022\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.3033\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.3018\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.3033\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.3016\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.3029\n",
      "Epoch: 001/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 1.93 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.3030\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.3045\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 2.3025\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.3029\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 2.3041\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 2.3019\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 2.3027\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 2.3034\n",
      "Epoch: 002/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 4.09 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 2.3025\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 2.3016\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 2.3021\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 2.3021\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 2.3027\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 2.3023\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 2.3018\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 2.3018\n",
      "Epoch: 003/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 6.05 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 2.3017\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 2.3024\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 2.3019\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 2.3044\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 2.3017\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 2.3026\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 2.3031\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 2.3024\n",
      "Epoch: 004/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 8.15 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 2.3035\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 2.3024\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 2.3022\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 2.3029\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 2.3028\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 2.3040\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 2.3022\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 2.3024\n",
      "Epoch: 005/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 10.25 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 2.3032\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 2.3026\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 2.3027\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 2.3018\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 2.3030\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 2.3034\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 2.3034\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 2.3025\n",
      "Epoch: 006/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 12.49 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 2.3026\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 2.3030\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 2.3025\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 2.3022\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 2.3022\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 2.3035\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 2.3031\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 2.3022\n",
      "Epoch: 007/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 14.46 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 2.3015\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 2.3026\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 2.3018\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 2.3031\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 2.3039\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 2.3027\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 2.3025\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 2.3025\n",
      "Epoch: 008/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 16.67 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 2.3025\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 2.3024\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 2.3020\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 2.3032\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 2.3032\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 2.3022\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 2.3018\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 2.3025\n",
      "Epoch: 009/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 18.79 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 2.3029\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 2.3022\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 2.3026\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 2.3020\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 2.3025\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 2.3026\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 2.3026\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 2.3023\n",
      "Epoch: 010/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 20.71 min\n",
      "Total Training Time: 20.71 min\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelSigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44c6ef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 0.9472\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.0687\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.6920\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.7916\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.5357\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.4201\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.7329\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.7121\n",
      "Epoch: 001/010 | Train: 53.588% |  Loss: 1.368\n",
      "Time elapsed: 1.89 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.3462\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.5113\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.3508\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.3032\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.4003\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.2646\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.4495\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.2735\n",
      "Epoch: 002/010 | Train: 58.660% |  Loss: 1.319\n",
      "Time elapsed: 3.82 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.4839\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.1695\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.3182\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.9508\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.0859\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.0944\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.0103\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.3318\n",
      "Epoch: 003/010 | Train: 68.466% |  Loss: 0.939\n",
      "Time elapsed: 5.83 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.9668\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.0969\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.0289\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.1141\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.0795\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.1409\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.1673\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.0434\n",
      "Epoch: 004/010 | Train: 68.186% |  Loss: 0.968\n",
      "Time elapsed: 7.77 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.1051\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.1046\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.1554\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.2878\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.9232\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.9243\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.0644\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.2840\n",
      "Epoch: 005/010 | Train: 69.482% |  Loss: 0.947\n",
      "Time elapsed: 9.92 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.8387\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.8929\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.9278\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.2101\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.1130\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.8885\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.0717\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.9924\n",
      "Epoch: 006/010 | Train: 66.728% |  Loss: 1.134\n",
      "Time elapsed: 11.86 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.2102\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.8947\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.8592\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 1.0076\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.0951\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.8780\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.8733\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.0547\n",
      "Epoch: 007/010 | Train: 69.316% |  Loss: 0.972\n",
      "Time elapsed: 13.76 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.1278\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.9567\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.9398\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.8577\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.8197\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.8083\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 1.2194\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.0398\n",
      "Epoch: 008/010 | Train: 74.374% |  Loss: 0.791\n",
      "Time elapsed: 15.72 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.7757\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 1.0923\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.9829\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.9008\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.1752\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.8782\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.7331\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.7704\n",
      "Epoch: 009/010 | Train: 74.334% |  Loss: 0.825\n",
      "Time elapsed: 17.68 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.6718\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.8654\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.8264\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.7935\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.6767\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 1.0087\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 1.0075\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.8339\n",
      "Epoch: 010/010 | Train: 76.922% |  Loss: 0.733\n",
      "Time elapsed: 19.64 min\n",
      "Total Training Time: 19.64 min\n",
      "Test accuracy: 68.32%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelTanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a15a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetRelu3(models.AlexNet):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetRelu3, self).__init__(num_classes=num_classes)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 4096), # Adjusted for the new feature map size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class AlexNetSigmoid3(models.AlexNet):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetSigmoid3, self).__init__(num_classes=num_classes)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 4096), # Adjusted for the new feature map size\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class AlexNetTanh3(models.AlexNet):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetTanh3, self).__init__(num_classes=num_classes)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 4096), # Adjusted for the new feature map size\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "modelSigmoid3 = AlexNetSigmoid3(num_classes=num_classes).to(device)\n",
    "modelRelu3 = AlexNetRelu3(num_classes=num_classes).to(device)\n",
    "modelTanh3 = AlexNetTanh3(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803909fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3054\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.2941\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.2801\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.2468\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.2037\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.9062\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.9982\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.9240\n",
      "Epoch: 001/010 | Train: 34.644% |  Loss: 1.838\n",
      "Time elapsed: 2.14 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.8127\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.7773\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.8048\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.8110\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.4912\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.6845\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.5083\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.5099\n",
      "Epoch: 002/010 | Train: 44.582% |  Loss: 1.553\n",
      "Time elapsed: 4.37 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.5678\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.6091\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.4734\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.4951\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.4950\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.7002\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.4922\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.3908\n",
      "Epoch: 003/010 | Train: 49.758% |  Loss: 1.410\n",
      "Time elapsed: 6.42 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.5466\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.2717\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.3831\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.3307\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.4487\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.2785\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.3359\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.3273\n",
      "Epoch: 004/010 | Train: 54.006% |  Loss: 1.305\n",
      "Time elapsed: 8.50 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.2475\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.1120\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.3196\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.3497\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.4417\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.1843\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.2981\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.4170\n",
      "Epoch: 005/010 | Train: 58.574% |  Loss: 1.190\n",
      "Time elapsed: 10.51 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.1656\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.1794\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.0735\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.2045\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.3257\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.2762\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.1809\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.0323\n",
      "Epoch: 006/010 | Train: 60.076% |  Loss: 1.139\n",
      "Time elapsed: 12.47 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.3103\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.0990\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.1023\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 1.1483\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.1207\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.3575\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.1552\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.1620\n",
      "Epoch: 007/010 | Train: 61.110% |  Loss: 1.115\n",
      "Time elapsed: 14.40 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.1039\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.0671\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 1.0638\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 1.0837\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.8890\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 1.0157\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 1.0646\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.0768\n",
      "Epoch: 008/010 | Train: 65.920% |  Loss: 0.980\n",
      "Time elapsed: 16.31 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 1.0112\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.9519\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.9240\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 1.0784\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.0294\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 1.1337\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.9615\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 1.0020\n",
      "Epoch: 009/010 | Train: 69.894% |  Loss: 0.894\n",
      "Time elapsed: 18.47 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.9502\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.8346\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.8928\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.7493\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.8653\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 1.0125\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.9640\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 1.0876\n",
      "Epoch: 010/010 | Train: 70.978% |  Loss: 0.843\n",
      "Time elapsed: 20.58 min\n",
      "Total Training Time: 20.58 min\n",
      "Test accuracy: 66.64%\n"
     ]
    }
   ],
   "source": [
    "eval(modelRelu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b0fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3680\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3368\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.2838\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.4259\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.3594\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.3480\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.3475\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.4009\n",
      "Epoch: 001/010 | Train: 10.000% |  Loss: 2.310\n",
      "Time elapsed: 2.01 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.3472\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.3228\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 2.3210\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.3439\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 2.3871\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 2.3384\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 2.3055\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 2.3715\n",
      "Epoch: 002/010 | Train: 10.000% |  Loss: 2.321\n",
      "Time elapsed: 4.12 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 2.3730\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 2.3198\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 2.3386\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 2.3101\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 2.3355\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 2.3356\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 2.3516\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 2.3305\n",
      "Epoch: 003/010 | Train: 10.000% |  Loss: 2.313\n",
      "Time elapsed: 6.36 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 2.3376\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 2.3816\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 2.3416\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 2.2990\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 2.3294\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 2.3321\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 2.3126\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 2.3123\n",
      "Epoch: 004/010 | Train: 10.000% |  Loss: 2.317\n",
      "Time elapsed: 8.55 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 2.3478\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 2.3698\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 2.3255\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 2.3257\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 2.3181\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 2.3197\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 2.3184\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 2.3371\n",
      "Epoch: 005/010 | Train: 10.000% |  Loss: 2.308\n",
      "Time elapsed: 10.63 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 2.2927\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 2.3758\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 2.3269\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 2.3205\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 2.3172\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 2.3239\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 2.3134\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 2.3499\n",
      "Epoch: 006/010 | Train: 10.000% |  Loss: 2.309\n",
      "Time elapsed: 12.63 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 2.3289\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 2.3335\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 2.3271\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 2.3091\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 2.3139\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 2.3114\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 2.2930\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 2.3383\n",
      "Epoch: 007/010 | Train: 10.000% |  Loss: 2.322\n",
      "Time elapsed: 14.67 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 2.3631\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 2.3439\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 2.3063\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 2.3577\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 2.3107\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 2.3205\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 2.3130\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 2.3313\n",
      "Epoch: 008/010 | Train: 10.000% |  Loss: 2.309\n",
      "Time elapsed: 16.68 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 2.3185\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 2.3390\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 2.3298\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 2.2875\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 2.3552\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 2.3251\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 2.3178\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 2.3417\n",
      "Epoch: 009/010 | Train: 10.000% |  Loss: 2.308\n",
      "Time elapsed: 18.91 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 2.3272\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 2.3360\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 2.2818\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 2.3264\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 2.3044\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 2.2984\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 2.3184\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 2.2949\n",
      "Epoch: 010/010 | Train: 10.000% |  Loss: 2.317\n",
      "Time elapsed: 20.86 min\n",
      "Total Training Time: 20.86 min\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "eval(modelSigmoid3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbfe4752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3053\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.1677\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.1139\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.0365\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.8459\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.8306\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.7228\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.7928\n",
      "Epoch: 001/010 | Train: 39.318% |  Loss: 1.739\n",
      "Time elapsed: 2.04 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.7662\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.6521\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.6442\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.6619\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.6018\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.4843\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.3804\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.5466\n",
      "Epoch: 002/010 | Train: 47.696% |  Loss: 1.474\n",
      "Time elapsed: 3.98 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.3679\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.5595\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.4354\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.4914\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.4719\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.4295\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.4898\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.2722\n",
      "Epoch: 003/010 | Train: 52.316% |  Loss: 1.345\n",
      "Time elapsed: 5.86 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.4213\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.3885\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.3097\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.3409\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.3035\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.2059\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.2528\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.2943\n",
      "Epoch: 004/010 | Train: 56.578% |  Loss: 1.251\n",
      "Time elapsed: 7.72 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.2673\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.2484\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.2358\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.1797\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.4313\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.0964\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.1602\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.1843\n",
      "Epoch: 005/010 | Train: 58.730% |  Loss: 1.188\n",
      "Time elapsed: 9.63 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.2318\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.1058\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.2690\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.2454\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.1221\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.9819\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.9522\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.9869\n",
      "Epoch: 006/010 | Train: 61.324% |  Loss: 1.109\n",
      "Time elapsed: 11.44 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.2394\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.1254\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.0322\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.9790\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.1242\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.1606\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.9558\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.1717\n",
      "Epoch: 007/010 | Train: 64.592% |  Loss: 1.027\n",
      "Time elapsed: 13.24 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.1426\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.1065\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 1.1252\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 1.1493\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 1.0291\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 1.0599\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.9735\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.0248\n",
      "Epoch: 008/010 | Train: 65.068% |  Loss: 0.996\n",
      "Time elapsed: 15.04 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.8977\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 1.2943\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.9955\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.9142\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.1105\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.9198\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.9996\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.8496\n",
      "Epoch: 009/010 | Train: 67.390% |  Loss: 0.947\n",
      "Time elapsed: 16.83 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 1.0815\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.8648\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 1.0856\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 1.0391\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 1.0525\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.8925\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.9036\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.9207\n",
      "Epoch: 010/010 | Train: 69.984% |  Loss: 0.873\n",
      "Time elapsed: 18.62 min\n",
      "Total Training Time: 18.62 min\n",
      "Test accuracy: 68.35%\n"
     ]
    }
   ],
   "source": [
    "eval(modelTanh3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a406820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 0.8940\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 1.9560\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.7054\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.6646\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.5616\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.5437\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.7146\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.6170\n",
      "Epoch: 001/010 | Train: 47.372% |  Loss: 1.454\n",
      "Time elapsed: 1.92 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.6764\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.4033\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.5083\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.4032\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.5131\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.4282\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.4173\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.2690\n",
      "Epoch: 002/010 | Train: 49.496% |  Loss: 1.389\n",
      "Time elapsed: 3.84 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.3348\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.0317\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.1938\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.2829\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.1933\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.1813\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.2760\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.2023\n",
      "Epoch: 003/010 | Train: 58.590% |  Loss: 1.157\n",
      "Time elapsed: 5.80 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.2565\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.1393\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.2780\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.2061\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.1923\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.3620\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.2394\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.0276\n",
      "Epoch: 004/010 | Train: 61.058% |  Loss: 1.092\n",
      "Time elapsed: 7.75 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.9558\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.9528\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.0887\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.1028\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.1363\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.1630\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.1116\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.1430\n",
      "Epoch: 005/010 | Train: 62.344% |  Loss: 1.057\n",
      "Time elapsed: 9.72 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.1817\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.0817\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.0032\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.0420\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.9573\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.1248\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.9775\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.8580\n",
      "Epoch: 006/010 | Train: 66.770% |  Loss: 0.943\n",
      "Time elapsed: 11.56 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.9521\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.9629\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.9924\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.8212\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.0087\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.0573\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.2105\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.9681\n",
      "Epoch: 007/010 | Train: 68.972% |  Loss: 0.882\n",
      "Time elapsed: 13.80 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.8487\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.8520\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.9825\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.9478\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 1.0027\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.8014\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.9656\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.8856\n",
      "Epoch: 008/010 | Train: 70.984% |  Loss: 0.822\n",
      "Time elapsed: 15.93 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.9422\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.8791\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.8539\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.9046\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.8713\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.8100\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.9630\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.9319\n",
      "Epoch: 009/010 | Train: 73.070% |  Loss: 0.764\n",
      "Time elapsed: 17.97 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.5947\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.7709\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.8138\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.7817\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.7690\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.7276\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.9102\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.5977\n",
      "Epoch: 010/010 | Train: 77.278% |  Loss: 0.645\n",
      "Time elapsed: 19.88 min\n",
      "Total Training Time: 19.88 min\n",
      "Test accuracy: 70.35%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelRelu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55047b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3573\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.4746\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.4147\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.3974\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.3778\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.3816\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.4113\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.3469\n",
      "Epoch: 001/010 | Train: 10.000% |  Loss: 2.358\n",
      "Time elapsed: 1.88 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.4272\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.4495\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 2.4335\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.4093\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 2.2826\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 2.3196\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 2.1366\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 2.0566\n",
      "Epoch: 002/010 | Train: 24.780% |  Loss: 2.038\n",
      "Time elapsed: 3.77 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.9932\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.9915\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 2.0177\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.9392\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 2.0671\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.8640\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.9756\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.8208\n",
      "Epoch: 003/010 | Train: 35.020% |  Loss: 1.763\n",
      "Time elapsed: 5.74 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.7322\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.8723\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.7526\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.7335\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.8906\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.6342\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.7719\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.6161\n",
      "Epoch: 004/010 | Train: 45.486% |  Loss: 1.507\n",
      "Time elapsed: 7.63 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.4946\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.6755\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.6188\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.4729\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.5354\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.3914\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.6154\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.3587\n",
      "Epoch: 005/010 | Train: 50.274% |  Loss: 1.352\n",
      "Time elapsed: 9.60 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.4041\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.5887\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.6035\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.4281\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.4654\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.4325\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.2774\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.4842\n",
      "Epoch: 006/010 | Train: 52.866% |  Loss: 1.307\n",
      "Time elapsed: 11.59 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.7345\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.5261\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.3915\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 1.2942\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.1880\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.4124\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.3789\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.2107\n",
      "Epoch: 007/010 | Train: 55.856% |  Loss: 1.204\n",
      "Time elapsed: 13.52 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.5561\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.3584\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 1.2365\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 1.2162\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 1.1542\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 1.3645\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 1.2590\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.1565\n",
      "Epoch: 008/010 | Train: 60.718% |  Loss: 1.090\n",
      "Time elapsed: 15.51 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 1.3160\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.9716\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 1.1675\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 1.2462\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.0231\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 1.0629\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 1.1811\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 1.1660\n",
      "Epoch: 009/010 | Train: 63.386% |  Loss: 1.015\n",
      "Time elapsed: 17.60 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 1.1582\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.8694\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 1.1706\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.9467\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.9473\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 1.2241\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 1.2840\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 1.0543\n",
      "Epoch: 010/010 | Train: 64.134% |  Loss: 1.004\n",
      "Time elapsed: 19.62 min\n",
      "Total Training Time: 19.62 min\n",
      "Test accuracy: 61.57%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelSigmoid3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b19a5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 0.6787\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.1645\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.0785\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.9582\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.9908\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.5546\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.5170\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.5151\n",
      "Epoch: 001/010 | Train: 46.118% |  Loss: 1.595\n",
      "Time elapsed: 1.85 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.7041\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.5838\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.2711\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.4301\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.3687\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.2557\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.4340\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.4515\n",
      "Epoch: 002/010 | Train: 58.716% |  Loss: 1.209\n",
      "Time elapsed: 3.71 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.4961\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.5784\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.2185\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.1089\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.2976\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.2004\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.2797\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.0924\n",
      "Epoch: 003/010 | Train: 65.592% |  Loss: 0.975\n",
      "Time elapsed: 5.65 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.3916\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.1713\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.0517\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.2429\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.2057\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.0067\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.1889\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.0975\n",
      "Epoch: 004/010 | Train: 69.430% |  Loss: 0.880\n",
      "Time elapsed: 7.57 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.9855\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.2519\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.3730\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.3167\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.1486\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.3282\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.2467\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.0749\n",
      "Epoch: 005/010 | Train: 65.396% |  Loss: 1.052\n",
      "Time elapsed: 9.74 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.3861\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.8900\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.0831\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.1377\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.2892\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.1207\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.1511\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.1004\n",
      "Epoch: 006/010 | Train: 69.486% |  Loss: 0.882\n",
      "Time elapsed: 11.75 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.1184\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.8975\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.3417\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.8660\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.8987\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.2239\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.2429\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.2777\n",
      "Epoch: 007/010 | Train: 69.946% |  Loss: 0.886\n",
      "Time elapsed: 13.64 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 1.0503\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.1181\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.9577\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 1.3321\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 1.0730\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 1.1904\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 1.2630\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.3737\n",
      "Epoch: 008/010 | Train: 70.512% |  Loss: 0.892\n",
      "Time elapsed: 15.62 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 1.2501\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 1.1742\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.8898\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 1.2063\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.2400\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 1.0752\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.9524\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 1.1060\n",
      "Epoch: 009/010 | Train: 70.454% |  Loss: 0.884\n",
      "Time elapsed: 17.61 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 1.1218\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 1.3701\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.9766\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.9859\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 1.2165\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 1.0651\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.8977\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 1.0333\n",
      "Epoch: 010/010 | Train: 71.638% |  Loss: 0.830\n",
      "Time elapsed: 19.54 min\n",
      "Total Training Time: 19.54 min\n",
      "Test accuracy: 68.57%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelTanh3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8803bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = device = torch.device('mps')\n",
    "class AlexNetRelu7(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetRelu7, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 6 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class AlexNetSigmoid7(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetSigmoid7, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 6 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class AlexNetTanh7(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetTanh7, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 6 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "modelSigmoid7 = AlexNetSigmoid7(num_classes=num_classes).to(device)\n",
    "modelRelu7 = AlexNetRelu7(num_classes=num_classes).to(device)\n",
    "modelTanh7 = AlexNetTanh7(num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93c00d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3021\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3014\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.2973\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.2972\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.2909\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.2852\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.2583\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.1752\n",
      "Epoch: 001/010 | Train: 25.554% |  Loss: 2.053\n",
      "Time elapsed: 1.92 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.1098\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.0275\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.9134\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.0014\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.7741\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.8885\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.7573\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.6383\n",
      "Epoch: 002/010 | Train: 39.612% |  Loss: 1.671\n",
      "Time elapsed: 3.83 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.6484\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.7923\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.5840\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 1.6003\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.4442\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.5812\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.6593\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 1.5320\n",
      "Epoch: 003/010 | Train: 46.502% |  Loss: 1.491\n",
      "Time elapsed: 5.77 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 1.5243\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.4422\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 1.5654\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.5508\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.4740\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.3241\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.2621\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.4011\n",
      "Epoch: 004/010 | Train: 50.030% |  Loss: 1.393\n",
      "Time elapsed: 7.68 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 1.4673\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 1.3196\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.3173\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.3878\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.2824\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.4264\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 1.2108\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 1.2020\n",
      "Epoch: 005/010 | Train: 55.536% |  Loss: 1.263\n",
      "Time elapsed: 9.68 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.2790\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.3456\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.3841\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 1.1621\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 1.2973\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.2345\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 1.1492\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.2972\n",
      "Epoch: 006/010 | Train: 56.872% |  Loss: 1.215\n",
      "Time elapsed: 11.70 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 1.1019\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.2458\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 1.0491\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 1.0889\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.1288\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 1.0144\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 1.2979\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.2290\n",
      "Epoch: 007/010 | Train: 61.730% |  Loss: 1.096\n",
      "Time elapsed: 13.62 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.9846\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 1.0627\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 1.1677\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 1.0058\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.9812\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.9724\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.9782\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 1.2030\n",
      "Epoch: 008/010 | Train: 64.198% |  Loss: 1.025\n",
      "Time elapsed: 15.79 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 1.1828\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 1.1604\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 1.1201\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 1.0433\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.0426\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.9263\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.9135\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.9378\n",
      "Epoch: 009/010 | Train: 67.114% |  Loss: 0.948\n",
      "Time elapsed: 17.71 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 1.0862\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 1.0206\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 1.0146\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.9740\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 1.1333\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.9232\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.9118\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.9877\n",
      "Epoch: 010/010 | Train: 67.122% |  Loss: 0.960\n",
      "Time elapsed: 19.71 min\n",
      "Total Training Time: 19.71 min\n",
      "Test accuracy: 64.67%\n"
     ]
    }
   ],
   "source": [
    "eval(modelRelu7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f92d6b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.4099\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3247\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.3556\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.3624\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.3576\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.3188\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.3976\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.3273\n",
      "Epoch: 001/010 | Train: 10.000% |  Loss: 2.330\n",
      "Time elapsed: 2.04 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.3384\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.3717\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 2.2946\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.3236\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 2.2852\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 2.3580\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 2.3570\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 2.3346\n",
      "Epoch: 002/010 | Train: 10.000% |  Loss: 2.333\n",
      "Time elapsed: 4.10 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 2.3270\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 2.3593\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 2.2920\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 2.2992\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 2.3121\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 2.3252\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 2.3336\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 2.3643\n",
      "Epoch: 003/010 | Train: 10.000% |  Loss: 2.323\n",
      "Time elapsed: 6.17 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 2.2916\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 2.2987\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 2.3363\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 2.3258\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 2.3301\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 2.3189\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 2.3463\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 2.3472\n",
      "Epoch: 004/010 | Train: 10.000% |  Loss: 2.318\n",
      "Time elapsed: 8.15 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 2.3078\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 2.3009\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 2.3328\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 2.3015\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 2.3057\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 2.3232\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 2.3314\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 2.3130\n",
      "Epoch: 005/010 | Train: 10.000% |  Loss: 2.311\n",
      "Time elapsed: 10.24 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 2.3015\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 2.3139\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 2.3125\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 2.3029\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 2.3289\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 2.3011\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 2.3015\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 2.2933\n",
      "Epoch: 006/010 | Train: 10.000% |  Loss: 2.309\n",
      "Time elapsed: 12.27 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 2.3179\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 2.3067\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 2.3071\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 2.2993\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 2.3211\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 2.3174\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 2.2942\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 2.3193\n",
      "Epoch: 007/010 | Train: 10.000% |  Loss: 2.312\n",
      "Time elapsed: 14.21 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 2.3288\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 2.3250\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 2.3117\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 2.2935\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 2.3169\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 2.3475\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 2.3075\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 2.3273\n",
      "Epoch: 008/010 | Train: 10.000% |  Loss: 2.308\n",
      "Time elapsed: 16.22 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 2.3167\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 2.2993\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 2.3021\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 2.3181\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 2.2997\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 2.3320\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 2.3083\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 2.3086\n",
      "Epoch: 009/010 | Train: 10.000% |  Loss: 2.308\n",
      "Time elapsed: 18.20 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 2.3030\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 2.3094\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 2.3103\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 2.3280\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 2.3216\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 2.3161\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 2.3033\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 2.2979\n",
      "Epoch: 010/010 | Train: 10.000% |  Loss: 2.308\n",
      "Time elapsed: 20.22 min\n",
      "Total Training Time: 20.22 min\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "eval(modelSigmoid7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1b1c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 0.9859\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 1.1368\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 0.9076\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 0.9843\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 0.7018\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 0.6648\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 0.9376\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 0.6469\n",
      "Epoch: 001/010 | Train: 77.512% |  Loss: 0.640\n",
      "Time elapsed: 2.09 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 0.6750\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 0.7617\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 0.7171\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 0.8301\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 0.7827\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 0.8380\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 0.8389\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 0.6566\n",
      "Epoch: 002/010 | Train: 78.136% |  Loss: 0.615\n",
      "Time elapsed: 4.00 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 0.7245\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 0.7728\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 0.7044\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.7495\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 0.7872\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 0.6948\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 0.8663\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 0.7310\n",
      "Epoch: 003/010 | Train: 79.770% |  Loss: 0.574\n",
      "Time elapsed: 5.97 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.6488\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 0.6101\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 0.6988\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 0.9104\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 0.6427\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 0.6766\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 0.7458\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 0.5815\n",
      "Epoch: 004/010 | Train: 80.002% |  Loss: 0.572\n",
      "Time elapsed: 7.89 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.7560\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.6903\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 0.6572\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 0.7524\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.8862\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.6152\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 0.6883\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.7697\n",
      "Epoch: 005/010 | Train: 80.954% |  Loss: 0.544\n",
      "Time elapsed: 9.85 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.7395\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.6454\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.6473\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.6566\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.6477\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.6869\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.7403\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.7635\n",
      "Epoch: 006/010 | Train: 81.204% |  Loss: 0.536\n",
      "Time elapsed: 11.85 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.6258\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.6785\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.7412\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.5759\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.9142\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.5671\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.6207\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.6189\n",
      "Epoch: 007/010 | Train: 80.876% |  Loss: 0.545\n",
      "Time elapsed: 13.82 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.7445\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.5919\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.6304\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.5725\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.6342\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.7199\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.7098\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.6117\n",
      "Epoch: 008/010 | Train: 81.194% |  Loss: 0.533\n",
      "Time elapsed: 15.68 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.8056\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.7100\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.5722\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.6944\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.8078\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.8715\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.8276\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.5888\n",
      "Epoch: 009/010 | Train: 81.830% |  Loss: 0.521\n",
      "Time elapsed: 17.51 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.6477\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.7441\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.6748\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.5478\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.6733\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.6696\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.6920\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.7186\n",
      "Epoch: 010/010 | Train: 81.712% |  Loss: 0.519\n",
      "Time elapsed: 19.71 min\n",
      "Total Training Time: 19.71 min\n",
      "Test accuracy: 74.71%\n"
     ]
    }
   ],
   "source": [
    "eval(modelTanh3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d36c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 1.0177\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 1.7759\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.5127\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.4187\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.4691\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.3392\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.1345\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.1880\n",
      "Epoch: 001/010 | Train: 58.782% |  Loss: 1.155\n",
      "Time elapsed: 1.93 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.1452\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.0703\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.1536\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 0.9929\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.2124\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.1725\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 0.9862\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 0.9723\n",
      "Epoch: 002/010 | Train: 67.322% |  Loss: 0.931\n",
      "Time elapsed: 4.17 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.0065\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.0102\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.0923\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.9713\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 0.9746\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 0.8724\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 0.9219\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 0.8291\n",
      "Epoch: 003/010 | Train: 73.538% |  Loss: 0.766\n",
      "Time elapsed: 6.28 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.7693\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 0.6399\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 0.6631\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 0.7216\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 0.7400\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 0.6972\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 0.7143\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 0.7917\n",
      "Epoch: 004/010 | Train: 76.586% |  Loss: 0.677\n",
      "Time elapsed: 8.54 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.7062\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.8140\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 0.7166\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 0.7019\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.7204\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.6801\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 0.6768\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.5581\n",
      "Epoch: 005/010 | Train: 75.832% |  Loss: 0.698\n",
      "Time elapsed: 10.78 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.7569\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.6538\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.6691\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.7091\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.7237\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.4985\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.5251\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.4897\n",
      "Epoch: 006/010 | Train: 78.638% |  Loss: 0.613\n",
      "Time elapsed: 13.30 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.5780\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.6390\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.5856\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.6228\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.5959\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.7024\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.6045\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.6527\n",
      "Epoch: 007/010 | Train: 82.418% |  Loss: 0.510\n",
      "Time elapsed: 15.60 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.5006\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.5033\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.6502\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.3964\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.6229\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.4712\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.4347\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.5058\n",
      "Epoch: 008/010 | Train: 85.026% |  Loss: 0.428\n",
      "Time elapsed: 17.89 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.4243\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.6726\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.4605\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.5877\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.5767\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.5269\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.5097\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.3728\n",
      "Epoch: 009/010 | Train: 84.714% |  Loss: 0.435\n",
      "Time elapsed: 20.14 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.4352\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.3513\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.4976\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.2838\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.3880\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.4148\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.4178\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.5011\n",
      "Epoch: 010/010 | Train: 88.636% |  Loss: 0.331\n",
      "Time elapsed: 22.50 min\n",
      "Total Training Time: 22.50 min\n",
      "Test accuracy: 75.19%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelRelu7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98ca132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.2954\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 2.3033\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 2.3041\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 2.3038\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 2.3031\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 2.3029\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 2.3023\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 2.3019\n",
      "Epoch: 001/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 2.19 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 2.3027\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 2.3026\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 2.3034\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 2.3028\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 2.3027\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 2.3035\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 2.3035\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 2.3022\n",
      "Epoch: 002/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 4.43 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 2.3027\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 2.3022\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 2.3024\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 2.3038\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 2.3025\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 2.3035\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 2.3026\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 2.3027\n",
      "Epoch: 003/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 6.81 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 2.3025\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 2.3015\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 2.3023\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 2.3025\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 2.3025\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 2.3019\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 2.3025\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 2.3017\n",
      "Epoch: 004/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 9.07 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 2.3028\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 2.3029\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 2.3026\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 2.3042\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 2.3019\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 2.3024\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 2.3027\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 2.3027\n",
      "Epoch: 005/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 11.35 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 2.3027\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 2.3032\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 2.3032\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 2.3033\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 2.3031\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 2.3028\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 2.3024\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 2.3025\n",
      "Epoch: 006/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 13.65 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 2.3029\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 2.3028\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 2.3031\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 2.3034\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 2.3031\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 2.3014\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 2.3032\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 2.3027\n",
      "Epoch: 007/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 15.91 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 2.3026\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 2.3036\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 2.3020\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 2.3025\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 2.3022\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 2.3030\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 2.3035\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 2.3024\n",
      "Epoch: 008/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 18.20 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 2.3027\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 2.3026\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 2.3023\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 2.3029\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 2.3010\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 2.3028\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 2.3028\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 2.3024\n",
      "Epoch: 009/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 20.47 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 2.3036\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 2.3025\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 2.3031\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 2.3027\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 2.3027\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 2.3030\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 2.3019\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 2.3028\n",
      "Epoch: 010/010 | Train: 10.000% |  Loss: 2.303\n",
      "Time elapsed: 22.75 min\n",
      "Total Training Time: 22.75 min\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelSigmoid7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b62cd1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 0.7708\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 0.8933\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.0275\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.3089\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.0265\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 0.9178\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.2055\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.3093\n",
      "Epoch: 001/010 | Train: 71.300% |  Loss: 0.852\n",
      "Time elapsed: 2.12 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.2242\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.1395\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 0.8627\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.3960\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 0.8891\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 0.9814\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 0.8970\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 0.8781\n",
      "Epoch: 002/010 | Train: 72.448% |  Loss: 0.801\n",
      "Time elapsed: 4.29 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.1750\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.1012\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 0.9268\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.9700\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.0571\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.1069\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.2188\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 0.9743\n",
      "Epoch: 003/010 | Train: 73.432% |  Loss: 0.779\n",
      "Time elapsed: 6.46 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.8718\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 0.9920\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 0.9876\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 1.0841\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 1.1054\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 1.1374\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 0.7948\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 1.0720\n",
      "Epoch: 004/010 | Train: 74.138% |  Loss: 0.768\n",
      "Time elapsed: 8.83 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.6895\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.8102\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 1.0063\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 1.2510\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 1.3736\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 1.1882\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 0.9322\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.8487\n",
      "Epoch: 005/010 | Train: 73.502% |  Loss: 0.776\n",
      "Time elapsed: 11.11 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 1.0129\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 1.0110\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 1.1533\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.8793\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.9303\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 1.1252\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.8738\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 1.3230\n",
      "Epoch: 006/010 | Train: 73.810% |  Loss: 0.784\n",
      "Time elapsed: 13.40 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.9270\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 1.1853\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.8880\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.9063\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 1.1384\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.9404\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.9550\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 1.2537\n",
      "Epoch: 007/010 | Train: 72.426% |  Loss: 0.834\n",
      "Time elapsed: 15.65 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.8697\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.8261\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.9802\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 1.0559\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.8431\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 1.0836\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.8820\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.8341\n",
      "Epoch: 008/010 | Train: 73.062% |  Loss: 0.802\n",
      "Time elapsed: 17.93 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 1.3862\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.8924\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 1.0275\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.9872\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 1.0777\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.9802\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 1.1796\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.8836\n",
      "Epoch: 009/010 | Train: 71.170% |  Loss: 0.864\n",
      "Time elapsed: 20.24 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.7137\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.9583\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.9273\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 1.2430\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.6759\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 1.1681\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 1.0041\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 1.3457\n",
      "Epoch: 010/010 | Train: 73.636% |  Loss: 0.789\n",
      "Time elapsed: 22.48 min\n",
      "Total Training Time: 22.48 min\n",
      "Test accuracy: 68.30%\n"
     ]
    }
   ],
   "source": [
    "evalAdam(modelTanh3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ccf17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
